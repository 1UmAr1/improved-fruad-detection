col = "blue", lwd = 3, type = "l")
xSequence <- c(seq(summary(lm2)$coeff[2, 3], 5, length = 10),
sumary(lm2)$coeff[2, 3])
xSequence <- c(seq(summary(lm2)$coeff[2, 3], 5, length = 10),
summary(lm2)$coeff[2, 3])
ySequence <- c(dt(seq(summary(lm2)$coeff[2, 3], 5, length),
df = 8), 0)
ySequence <- c(dt(seq(summary(lm2)$coeff[2, 3], 5, length = 10),
df = 8), 0)
polygon(xSequence, ySequence, col = "red"); polygon(-xSequence, ySequence, col = "red")
set.seed(88); pValues <- rep(NA, 100)
for(i in 1:100){
xValues <- rnorm(20); yValues <- rnorm(20)
pValues[i] <- summary(lm(yValues ~ xValues)) $coeff[2, 4]
}
hist(pValues, col = "blue", main = "", freq = F, xlim = c(0, 1)); abline(h = 1, col = "red", lwd = 3)
hist(pValues, col = "blue", main = "", freq = F, xlim = c(0, 1)); abline(h = 1, col = "red", lwd = 3)
for(i in 1:100){
xValues <- rnorm(20); yValues <- 0.2 * xValues + rnorm(20)
pValues[i] <- summary(lm(yValues ~ xValues)) $coeff[2, 4]
}
hist(pValues, col = "blue", main = "", freq = F, xlim = c(0, 1)); abline(h = 1, col = "red", lwd = 3)
for(i in 1:100){
xValues <- rnorm(20); yValues <- 0.2 * xValues + rnorm(100)
pValues[i] <- summary(lm(yValues ~ xValues)) $coeff[2, 4]
}
for(i in 1:100){
xValues <- rnorm(100); yValues <- 0.2 * xValues + rnorm(100)
pValues[i] <- summary(lm(yValues ~ xValues)) $coeff[2, 4]
}
hist(pValues, col = "blue", main = "", freq = F, xlim = c(0, 1)); abline(h = 1, col = "red", lwd = 3)
download.file("http://www.rossmachance.com/iscam2/data.movies03RT.txt", destfile = "D:Data.0/AI/R/New BO/movies.txt")
download.file("http://www.rossmachance.com/iscam2/data.movies03RT.txt", destfile = "D:Data.0/AI/R/New BO/movies.txt")
download.file("http://www.rossmachance.com/iscam2/data.movies03RT.txt", destfile = "D:Data.0/AI/R/New BO/")
download.file("http://www.rossmachance.com/iscam2/data.movies03RT.txt", destfile = "D:Data.0/Ai/R/New BO/")
download.file("http://www.rossmachance.com/iscam2/data.movies03RT.txt", destfile = "D:Data.0/Ai/R/NewBO/")
download.file("http://www.rossmachance.com/iscam2/data.movies03RT.txt", destfile = "D:Data.0")
data <- read.csv(file = file.choose(), header = T)
lm1 <- lm(data$Amount ~ data$Class)
plot(data$Class, data$Amount, pch = 19, col = "blue")
lines(data$Class, lm1$fitted, lwd = 3, col = "red")
# cheap chooour
lm2 <- lm(data$V10 ~ data$Class)
plot(data$Class, data$V10, pch = 19)
lines(data$class, lm2$fitted, col = "blue", lwd = 3)
lines(data$Class, lm2$fitted, col = "blue", lwd = 3)
train <- data * 0.1
train <- data * 0.0
set.seed(111)
train <- sample(data)
set.seed(111)
train <- sample(data, size = 30000)
set.seed(111)
train <- sample(data, size = 31: 30000)
train <- sample(data, size = 31,30000)
summary(lm1)
train <- sample(data, size = 30000)
library(caTools)
set.seed(111)
edata <- sample.split(data, SplitRatio = .65)
edatax
edata
set.seed(42)
# Transform "Class" to factor to perform classification and rename levels to predict class probabilities (need to be valid R variable names)
data$Class <- as.numeric(data$Class)
#data$Class <- revalue(data$Class, c("0"="false", "1"="true"))
#data$Class <- factor(data$Class, levels(data$Class)[c(2, 1)])
# Create training and testing set with stratification (i.e. preserving the proportions of false/true values from the "Class" column)
train_index <- createDataPartition(data$Class, times = 1, p = 0.8, list = F)
library(caret)
set.seed(42)
# Transform "Class" to factor to perform classification and rename levels to predict class probabilities (need to be valid R variable names)
data$Class <- as.numeric(data$Class)
#data$Class <- revalue(data$Class, c("0"="false", "1"="true"))
#data$Class <- factor(data$Class, levels(data$Class)[c(2, 1)])
# Create training and testing set with stratification (i.e. preserving the proportions of false/true values from the "Class" column)
train_index <- createDataPartition(data$Class, times = 1, p = 0.8, list = F)
X_train <- data[train_index]
X_test <- data[!train_index]
y_train <- data$Class[train_index]
y_test <- data$Class[-train_index]
# Parallel processing for faster training
# Parallel processing for faster training
registerDoMC(cores = 6)
library(doSNOW)
registterDoSNow(cl)
registerDoSNow(cl)
# Parallel processing for faster training
cl <- makecluster(cores = 6)
library(DoSNOW)
# Parallel processing for faster training
cl <- makecluster(cores = 6, type = "SOCk")
# Parallel processing for faster training
cl <- makeCluster(cores = 6, type = "SOCk")
# Parallel processing for faster training
cl <- makeCluster(cores = 6)
library(doSNOW)
cl <- makeCluster(6, type = "SOCK")
# have to register < R does not have auto register for dosnow
registerDoSNOW(cl)
set.seed(42)
# Transform "Class" to factor to perform classification and rename levels to predict class probabilities (need to be valid R variable names)
data$Class <- as.numeric(data$Class)
#data$Class <- revalue(data$Class, c("0"="false", "1"="true"))
#data$Class <- factor(data$Class, levels(data$Class)[c(2, 1)])
# Create training and testing set with stratification (i.e. preserving the proportions of false/true values from the "Class" column)
train_index <- createDataPartition(data$Class, times = 1, p = 0.8, list = F)
X_train <- data[train_index]
X_test <- data[!train_index]
y_train <- data$Class[train_index]
y_test <- data$Class[-train_index]
# Parallel processing for faster tr
# Use 10-fold cross-validation
ctrl <- trainControl(method = "cv",
number = 10,
verboseIter = T,
classProbs = T,
sampling = "smote",
summaryFunction = twoClassSummary,
savePredictions = T)
summary(lm1)
library(glmnet)
md3 <- glm(formula = Class ~ ., famly = "binomial", data = X_train)
md3 <- glm(formula = Class ~  V17 + V12 + V14 + V10 + V16 +
V11 + V9 + V4 + V18 + V26, famly = "binomial", data = X_train)
md3 <- glm(formula = Class ~  V17 + V12 + V14 + V10 + V16 +
V11 + V9 + V4 + V18 + V26, data = X_train)
?envir
??envir
md3 <- glm(formula = Class ~  V17 + V12 + V14 + V10 + V16 +
V11 + V9 + V4 + V18 + V26, data = X_train)
impo <- data.frame(X_train$V17)
impo <- data.frame(X_train)
View(impo)
md3 <- glm(formula = Class ~. , data = X_train)
md3 <- glm(formula = Class ~., family = "binomial" data = X_train)
anova(lm)
anova(lm1)
anova(lm1)
anoob <- aov(data$Amount ~ data$Class)
anoob
anoob$coefficients
anob <- aov(data$V1 ~ data$Time)
anob
anob$coefficients
anoob <- aov(data$Class ~ data$Amount)
anoob
anoob$coefficients
anob <- aov(data$Class ~ data$Amount + data$Time)
anob
anob$coefficients
summary(anob)
set.seed(111)
train1 <- sample(data[1:150000, ])
test <- sample(data[150001:280000, ])
head(train1)
test <- as.data.frame(test)
lmt <- lm(train1$Class ~ train1$V14)
summary(lm)
summary(lmt)
anob <- aov(data$Class ~ data$Amount + data$Time)
anob$coefficients
lmt2 <- lm(train1$Class ~ train$V17 + train$V4)
summary(train$V17)
summary(train1$V17)
lmt2 <- lm(train1$Class ~ train1$V17 + train1$V4)
summary(lmt2)
lmt2 <- lm(train1$Class ~  train1$V4)
summary(lmt2)
remove(train)
n <- train1$V4(any(train$Class == 1))
n <- train1$V4(which(train1$Class == 1))
lenght(train$V4)
length(train$V4)
length(train1$V4)
lm3 <- lm(train1$Class ~ train1$V10)
summary(lm3)
plot(train1$Class, train1$V4)
summary(train$Class ~ train$V4)
set.seed(111)
train <- sample(data[1:150000, ])
test <- sample(data[150001:280000, ])
head(train)
train <- as.data.frame(train1)
test <- as.data.frame(test)
lmt <- lm(train$Class ~ train$V14)
summary(lmt)
#sucks
lmt2 <- lm(train$Class ~  train$V4)
summary(lmt2)
plot(train$Class, train$V4)
?which
library(glmnet)
logis <- glm(train$Class ~  train$V4, fimaly = "binomial")
logis <- glm(train$Class ~  train$V4, family = "binomial")
logis
summary(logis)
plot(logis)
logis <- glm(train$Class ~  train$V4, family = "binomial")
plot(logis)
plot(logis)
plot(logis)
exp(confint(loglogis))
exp(loglogis)
exp(logis$coeff)
exp(confint(logis))
?predict
md <- predict(lm3, test)
md3
md
summary(md)
plot(md)
?ConfusionMatrix
plot(data$Class ~ train$V14)
plot(train$Class ~ train$V14)
abline(lmt)
summary(logis)
logiss <- glm(train$Class ~ ., data = train family = "binomial")
logiss <- glm(train$Class ~ ., data = train, family = "binomial")
summary(logiss)
predi <- data.frame(prof = logiss$fitted.values, cls = train$Class)
predi
plot(predi)
library("cowplot")
install.packages("cowplot")
predi.data$rank <- 1:nrow(predi.data)
predi.data$rank <- 1:nrow(predi)
predi$rank <- 1:nrow(predi)
library(ggplot2)
library(cowplot)
ggplot(data = predi, aes(x = rank, y = prof)) +
geom_point(aes(color = predi), alpha = 1, shape = 4, stroke = 2) +
theme_set(theme_cowplot()) +
ggplot(data = predi, aes(x = rank, y = prof)) +
geom_point(aes(color = predi), alpha = 1, shape = 4, stroke = 2) +
theme_set(theme_cowplot())
library(ggplot2)
library(cowplot)
ggplot(data = predi, aes(x = rank, y = prof)) +
geom_point(aes(color = predi), alpha = 1, shape = 4, stroke = 2) +
theme_set(theme_cowplot())
ggplot(data = predi, aes(x = rank, y = prof)) +
geom_point(alpha = 1, shape = 4, stroke = 2) +
theme_set(theme_cowplot())
head(data)
#checking the class of the dataset
class(data)
#checking the class of the variables using sapply
sapply(data[1,], class)
unique(data)
#checking the length of the unique vlaues of varaible class
length(unique(data$Class))
#doing the same with variable time and amount
unique(data$Time)
length(unique(data$Time))
unique(data$Amount)
length(unique(data$Amount))
#checking the relation between amount and class variable
table(data$Class, data$Amount)
#checking the relation between amount and class variable
table(data$Class[1:300], data$Amount[1:300])
#checking if their is any wrong value in class variable
any(data$Class[] > 1 || data$Class[] < 0)
#checking for any missing values in class variable
is.na(any(data$class))
#subsetting the class variable
data[data$Class[] == 0 & data$Class[] == 1, c("legit", "fruad")]
#checking if we have missing values in our entire dataset
is.na(any(data))
fruad
# Exploratory analysis
# taking a look at the entire data set
data
# summarizing the data
# checking the column names of the data
colnames(data)
# checking the number of rows
nrow(data)
# checking the dimensions of the data
dim(data)
summary(data)
# usnig quantile to check the summary of individual variables
# it aslo gives us an overview of the variables
quantile(data$Time)
quantile(data$Class)
quantile(data$Amount)
# checking the class of the variables using sapply
# seleting the first row and applying class function to the variables of
# first row using sapply
sapply(data[1,], class)
# checking the unique values of the Class variables
unique(data$Class)
# checking the length of the unique vlaues of varaible class
length(unique(data$Class))
# doing the same with variable time and amount
unique(data$Time)
length(unique(data$Time))
unique(data$Amount)
length(unique(data$Amount))
# table listed all the unique variables and the number of times it appeared
table(data$Class)
# here we have only 492 positive or fruads in the entire dataset
# as the dataset is the dependent variable
# checking the relation between first 300 observation
# of amount and class variable
table(data$Class[1:300], data$Amount[1:300])
# it shows the number of time a particular amount has been debited
# by the customers
# checking if their is any wrong value in class variable
any(data$Class[] > 1 | data$Class[] < 0)
# checking for any missing values in class variable
is.na(any(data$class))
# we have got no missing values in the Class variable which is a
# good thing
head(data, 2)
#checking if we have missing values in our entire dataset
is.na(any(data))
# summarizing the data
# checking the column names of the data
colnames(data)
# data munging
split.screen(plot(data$Time, data$V1))
# data munging
data$Amount[1:10]
loggis <- glm(train$Class ~ V26 + V18 + V19 + V2 + V16 + V12 + V3 + V4 + Amount + Time, data = train,
family = "binomial")
summary(loggis)
loggis <- glm(train$Class ~ V26 + V18 + V2 + V3 + V4 + Amount + Time, data = train,
family = "binomial")
summary(loggis)
loggis <- glm(train$Class ~ V26 + V18 +  V2 +  V3 + V4 + Amount + Time, data = train,
fimily = "binomial")
loggis <- glm(train$Class ~ V26 + V18 +  V2 +  V3 + V4 + Amount + Time, data = train,
family = "binomial")
summary(loggis)
plot(predi)
install.packages("pscl")
library(pscl)
pR2(loggis)
modelm <- predict(loggis, test, type = "response")
model.results <- ifesle(model.results > 0.5, 1, 0)
model.results <- ifelse(model.results > 0.5, 1, 0)
modelm <- ifelse(modelm > 0.5, 1, 0)
mis <- mean(modelm != test$Class)
print(paste("Accuracy", 1-mis))
logiss <- glm(train$Class ~ ., data = train, family = "binomial")
summary(logiss)
modeln <- predict(logiss, test, type = "response")
modeln <- ifelse(modelm > 0.5, 1, 0)
miss <- mean(modeln  != test$Class)
print(paste("Accuracy", 1-miss))
#logistic regression using all the variables
logal <- glm(train$Class ~ ., data = train, family = "binomial")
logal <- glm(train$Class ~ ., data = train, family = "binomial")
modelg <- predict(logal, test, type = "response")
modelg <- ifelse(modelg > 0.5, 1, 0)
mist <- mean(modelg != test$Class)
print(paste("Accuracy", 1-mist))
print(paste("Accuracy", 1-mist))
# model 2
loggist <- glm(train$Class ~ V26 + V18 +  V2 +  V3 + V4 + Amount, data = train,
family = "binomial")
modeltk <- predict(loggist, test, type = "response")
modeltk <- ifelse(modeltk > 0.5, 1, 0)
misti <- mean(modeltk != test$Class)
print(paste("Accuracy", 1-misti))
print(paste("Accuracy", 1-mis))
print(paste("Accuracy", 1-mist))
print(paste("Accuracy", 1-misti))
# model 5
loggist <- glm(train$Class ~ V26 + V18 +  V2 +  V3 + V4 + V16+ Amount + Time, data = test, family = "binomial")
# model 5
loggist <- glm(train$Class ~ V26 + V18 +  V2 +  V3 + V4 + V16 + Amount + Time, data = test, family = "binomial")
# model 5
loggist <- glm(train$Class ~ V26 + V18 +  V2 +  V3 + V4 + V16 + Amount + Time, data = train, family = "binomial")
# model 4
loggist <- glm(train$Class ~ V26 + V18 +  V2 +  V3 + V4 + Amount, data = train,
family = "binomial")
# model 5
loggi <- glm(train$Class ~ V26 + V18 +  V2 +  V3 + V4 + V16 + Amount + Time, data = train, family = "binomial")
modelk <- predict(loggi, test, type = "response")
modelk <- ifelse(modelk > 0.5, 1, 0)
modelk <- ifelse(modelk > 0.5, 1, 0)
mistk <- mean(modelk != test$Class)
print(paste("Accuracy", 1-mistk))
#best variables = v26, v19, v18, v4, v2, v16, v12, v3, amount,
# v22, v21, v17, v1, time
modelf <- predict(loggf, test, type = "response")
# model 6
loggf <- glm(train$Class ~ V26 +V19 + V12 + + V22 + V17 + V1 + V18 +  V2 +  V3 + V4 + V16 + Amount + Time, data = train, family = "binomial")
#best variables = v26, v19, v18, v4, v2, v16, v12, v3, amount,
# v22, v21, v17, v1, time
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~ V26 +V19 + V12 +  V22 + V17 + V1 + V18 +  V2 +  V3 + V4 + V16 + Amount + Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~ V26 +V19 + V12 +  V17 + V1 + V18 +  V2 +  V3 + V4 + V16 + Amount + Time, data = train, family = "binomial")
# v22, v21, v17, v1, time
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~ V26 +V19 +  V17 + V1 + V18 +  V2 +  V3 + V4 + V16 + Amount + Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V19 + V12 +  V17 + V1 + V18 +  V2 +  V3 + V4 + V16 + Amount + Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V19 + V12 +  V17 + V18 +  V2 +  V3 + V4 + V16 + Amount + Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V19 + V12 +  V17 + V1 +  V2 +  V3 + V4 + V16 + Amount + Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V19 + V12 +  V17 + V1 +  V2 +  V3 + V4 + V16 +  Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V19 + V12 +  V17 + V1 +  V2 +  V3 + V4 + V16 , data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V19 + V12 +  V17 + V1 +  V2 +  V3 + V4 + V16 +  Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V12 +  V17 + V1 +  V2 +  V3 + V4 + V16 +  Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V12 +   V1 +  V2 +  V3 + V4 + V16 +  Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V12 +  V17 +  V2 +  V3 + V4 + V16 +  Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V12 +  V17 + V1 +    V3 + V4 + V16 +  Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V12 +  V17 + V1 + V3 + V4 + V16 +  Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V12 +  V17 + V1 + V3 + V16 +  Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V12 +  V17 + V1 + V3 + V4 +   Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
# model 6
loggf <- glm(train$Class ~  V12 +  V17 + V1 + V3 + V4 + V16 +  Time, data = train, family = "binomial")
modelf <- predict(loggf, test, type = "response")
modelf <- ifelse(modelf > 0.5, 1, 0)
mistf <- mean(modelf != test$Class)
print(paste("Accuracy", 1-mistf))
modela <- predict(loggf, data, type = "response")
modela <- ifelse(modela > 0.5, 1, 0)
mista <- mean(modela != test$Class)
print(paste("Accuracy", 1-mista))
# training a neural net using same varaibles
library(neuralnet)
nn <- neuralnet(train$Class ~ V12 +  V17 + V1 + V3 + V4 + V16 +  Time, data = train,
hidden = c(10, 3))
plot(nn)
neup <- predict(nn, test, type = "response")
neup  <- ifelse(neup > 0.5, 1, 0)
mistn <- mean(neup != test$Class)
print(paste("Accuracy", 1-mistn))
